\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
% \usepackage{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{emoji}


\title{Proposal for a Workshop on \\
Causality and Large Models (C\emoji{heart}LM)}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
    Ching Lam Choi \thanks{Correspondence: \texttt{calmworkshop2024@gmail.com}} \thanks{MIT}
    \And
    Felix Leeb \footnotemark[1] \thanks{Max Planck Institute for Intelligent Systems}
    \And
    Luigi Gresele \thanks{University of Copenhagen}
    \And
    Josef Valvoda \footnotemark[4]
    \And
    Andrei Nicolicioiu \thanks{Mila}
    \And
    Xiusi Li \footnotemark[5]
    \And
    Patrik Reizinger \footnotemark[3]
    \And
    Sophie Xhonneux \footnotemark[5]
    \And
    Bernhard Schölkopf \footnotemark[3]
    \And
    Dhanya Sridhar \footnotemark[5]
    % \And % (waiting for response)
    % Nichlas Papernot
}


\begin{document}


\maketitle


\begin{abstract}
[todo]
\end{abstract}


% https://neurips.cc/Conferences/2024/WorkshopsGuidance

\section{Topic}

The remarkable capabilities and accessibility of recent large models, also known as ``foundation models'', have sparked significant interest and excitement in the research community and beyond. In particular large generative models have demonstrated remarkable competencies in understanding and generating human-like text, images, and more despite being trained on largely unstructured data using relatively simple self-supervised learning methods. This begs the question: "Why do such large models work so well?" % for, in
% instruction following, agentic behaviors

The impressive performance, sometimes even exceeding human experts, across a wide variety of benchmarks together with the incorporation of multiple modalities such as images, text, audio, etc. makes these large models into versatile decision-making systems and agents.
However, the increased adoption of these models is not without its challenges. The increasing size and complexity of these "black box" models raises concerns about their trustworthiness and reliability.
This is especially pertinent as these models find applications in high-stakes domains such as healthcare and policy-making, where decisions have significant real-world impacts.
This begs the question: "Under what circumstances can we trust these large models?" % with, of

Enter causality. Causal inference focuses on formalizing such questions and developing tools to answer them in a principled way. For example, by systematically identifying failure modes 
% or by isolating the model components that cause problematic behavior 
we can gain insights into the causal factors that affect the model's behavior. 
This can lead to more interpretable and explainable models, while also promising to improve model generalization and robustness by avoiding falling prey to spurious correlations and non-causal biases in the data.

Our workshop will explore how the rich framework of causal inference can be used to make sense of the impressive capabilities of large models. 
A growing area of research in this direction is to investigate the causal knowledge captured in large models despite being trained on unstructured, observational data. 
Past similar workshops, such as the Causal Representation Learning workshop at NeurIPS 2023, have strived to make causal methods more accessible to the machine learning community, and we hope to carry on this mission by discussing how a causal framing can enable more principled approaches and potentially even new capabilities such as systematic generalization or robustness. However, we hope to focus on the intersection between causality and large models in particular, as the paradigm shift towards large models has brought new challenges and opportunities that are rife for deeper discussion.
% For example, how this knowledge is utilized for "amortized causal reasoning" for solving complex tasks.
By engaging both theoretical and applied perspectives, we aim to foster deep insights into the behavior of large models, not only to improve the model's performance but also to make the behavior more interpretable and reliable.

[specific criteria (for call for papers)]

[list of "selected references"]

\section{Schedule and Speakers}

[note on diversity]

\subsection{Interactive Elements}

[equipment needs]

\section{Attendance}

[comparison to CRL]

\section{Team}

[introduction and affiliations]

Ching Lam Choi [MIT], Felix Leeb [MPI], Luigi Gresele [KU], Josef Valvoda [KU],  Andrei Nicolicioiu [Mila], Xiusi Li [Mila], Patrik Reizinger [MPI], Sophie Xhonneux [Mila]
PIs (2): Bernhard Schölkopf [MPI], Dhanya Sridhar [Mila]

Bio:\\

Ching Lam Choi is an incoming PhD student at MIT EECS; she has also spent time with brilliant mentors and peers at Mila, Max Planck Institute (MPI-IS Tübingen), University of Copenhagen, CUHK MMLab and more. Her experiences inspire her to advocate for diverse research participation, via hosting open office hours and co-organising ``New in ML Workshop" at NeurIPS 2023, ``CoSubmitting Summer Workshop" at ICLR 2022, and ``Undergraduates in Computer Vision Social" in ICCV 2021. Her research interests are in responsibly robust model scaling, through better synthetic data, causal reasoning capabilities and post-hoc fairness guarantees.

Felix Leeb is a PhD student at the Max Planck Institute for Intelligent Systems in Germany, under the supervision of Bernhard Schölkopf working on causal representation learning and evaluating the reasoning abilities of large language models. Before living in Germany, Felix completed his Bachelors in Physics, Chemistry, and Computer Science at the University of Washington in Seattle, USA, researching object pose estimation visuomotor control and using molecular dynamics simulations to study the structure of cell membranes.

Andrei Nicolicioiu is a PhD student at Mila and Université de Montréal, Canada with a Bachelor’s and a Master’s degree from the University Politehnica of Bucharest, Romania. Previously he worked as an ML researcher for 5 years at Bitdefender, Romania, and spent some time at the Max Planck Institute for Intelligent Systems, Germany. His research involves vision-language models with a focus on visual relational reasoning, out-of-distribution generalization and causally inspired representation learning, and data-selection methods.

[program committee and discussion of reviewing process]

[number of organizers] dedicated specialists (deep bench of experience); we are bringing two different communities together. organizers need to cover not just one but two communities (and everything in between)

\section{References}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

\appendix

\section{Appendix / supplemental material}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}