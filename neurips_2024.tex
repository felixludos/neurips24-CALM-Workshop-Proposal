\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
% \usepackage{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
% \usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
   \usepackage[nonatbib,preprint]{neurips_2024}

\usepackage[numbers]{natbib}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{emoji}


\title{Proposal for a Workshop on \\
Causality and Large Models (C\emoji{heart}LM)}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
    Ching Lam Choi \thanks{Correspondence: \texttt{calmworkshop2024@gmail.com}} \thanks{MIT}
    \And
    Felix Leeb \footnotemark[1] \thanks{Max Planck Institute for Intelligent Systems}
    \And
    Luigi Gresele \thanks{University of Copenhagen}
    \And
    Josef Valvoda \footnotemark[4]
    \And
    Andrei Nicolicioiu \thanks{Mila}
    \And
    Xiusi Li \footnotemark[5]
    \And
    Patrik Reizinger \footnotemark[3]
    \And
    Sophie Xhonneux \footnotemark[5]
    \And
    Bernhard Schölkopf \footnotemark[3]
    \And
    Dhanya Sridhar \footnotemark[5]
    % \And % (waiting for response)
    % Nichlas Papernot
}


\begin{document}


\maketitle


\begin{abstract}
[todo - based on topic section]
\end{abstract}


% https://neurips.cc/Conferences/2024/WorkshopsGuidance

\section{Topic}

The remarkable capabilities and accessibility of recent large models, also known as ``foundation models'', have sparked significant interest and excitement in the research community and beyond. In particular, large pre-trained generative models have demonstrated remarkable competencies in understanding and generating human-like text, images, and more despite being trained on largely unstructured data using relatively simple self-supervised learning methods. This raises the question: \textit{Why do such large models work so well?} % for, in
% instruction following, agentic behaviors

Furthermore, the impressive performance, sometimes even exceeding human experts, across a wide variety of benchmarks together with the incorporation of multiple modalities such as images, text, audio, etc. makes these large models into versatile decision-making systems and agents.
However, the increased adoption of these models is not without its challenges. The increasing size and complexity of these ``black box'' models raises concerns about their trustworthiness and reliability.
This is especially pertinent as these models find applications in high-stakes domains such as healthcare and policy-making, where decisions have significant real-world impacts.
Consequently, we must consider: \textit{Under what circumstances can we trust these large models?} % with, of

Enter causality. Causal inference focuses on formalizing such questions and developing tools to answer them in a principled way. For example, by systematically identifying failure modes 
% or by isolating the model components that cause problematic behavior 
we can gain insights into the causal factors that affect the model's behavior. 
This can lead to more interpretable and explainable models, while also promising to improve model out-of-distribution generalization and robustness by avoiding falling prey to spurious correlations and non-causal biases in the data.

Our workshop will explore the many exciting synergies between causality and large models. For instance, how the rich framework of causal inference can be used to make sense of the impressive capabilities of large models. 
A growing area of research in this direction is to investigate the causal knowledge captured in large models despite being trained on unstructured, observational data. 
Past similar workshops, such as the Causal Representation Learning workshop at NeurIPS 2023, have strived to make causal methods more accessible to the machine learning community, and we hope to carry on this mission by discussing how a causal framing can enable more principled approaches and potentially even new capabilities such as systematic generalization or robustness. However, we hope to focus on the intersection between causality and large models in particular, as the paradigm shift towards large models has brought new challenges and opportunities that are rife for deeper discussion.
% For example, how this knowledge is utilized for "amortized causal reasoning" for solving complex tasks.
By engaging both theoretical and applied perspectives, we aim to foster deep insights into the behavior of large models, not only to improve the model's performance but also to make the behavior more interpretable and reliable.

% crystallize emerging subfields
% 

[specific criteria (for call for papers)]

% [list of "selected references"]

\section{Schedule and Speakers}

We plan to hold a series of talks given by both up-and-coming as well as distinguished researchers with experience

We are fortunate to draw our speakers and panelists not just from one, but from two thriving and diverse research communities: large-scale machine learning and causal inference.

We also selected several notable researchers that focus on adjacent or applied fields to provide our attendees with insights in how state-of-the-art methods can rapidly and effectively be deployed to solve problems.

features a distinguished group of speakers and panelists. We have invited  Matej Zečević (Speaker), Maria Antoniak (Panelist), Giambattista Parascandolo (Panelist, tentatively accepted), Elias Baremboim (Speaker), Jane X. Wang (Speaker), Claudia Shi (Speaker), Victor Veitch (Speaker), Amit Sharma (Speaker), Atticus Geiger (Panelist), Charlotte Bunne (Panelist), Chelsea Finn (Panelist), and Zhijing Jin (Panelist).

[note on diversity]

\subsection{Interactive Elements}

% [equipment needs]

[lunch buddy program] to connect not just junior+senior people for lunch but also connecting different communities
    To connect junior and senior researchers and simultaneously encourage cooperation between the causality and large model communities, we---inspired by the success of a similar initiative at the NeurIPS workshop NeurReps---plan to setup a ``lunch buddy" system between the participants, making our workshop more interactive.

\section{Attendance}

[comparison to CRL]

\section{Team}

[introduction and affiliations]

Ching Lam Choi [MIT], Felix Leeb [MPI], Luigi Gresele [KU], Josef Valvoda [KU],  Andrei Nicolicioiu [Mila], Xiusi Li [Mila], Patrik Reizinger [MPI], Sophie Xhonneux [Mila]
PIs (2): Bernhard Schölkopf [MPI], Dhanya Sridhar [Mila]

Bio:\\

Ching Lam Choi is an incoming PhD student at MIT EECS; she has also spent time with brilliant mentors and peers at Mila, Max Planck Institute (MPI-IS Tübingen), University of Copenhagen, CUHK MMLab and more. Her experiences inspire her to advocate for diverse research participation, via hosting open office hours and co-organising ``New in ML Workshop" at NeurIPS 2023, ``CoSubmitting Summer Workshop" at ICLR 2022, and ``Undergraduates in Computer Vision Social" in ICCV 2021. Her research interests are in responsibly robust model scaling, through better synthetic data, causal reasoning capabilities and post-hoc fairness guarantees.

\href{https://felixludos.com/}{Felix Leeb} 
is a PhD student at the Max Planck Institute for Intelligent Systems in Germany, under the supervision of Bernhard Schölkopf working on causal representation learning and evaluating the reasoning abilities of large language models. Before living in Germany, Felix completed his Bachelors in Physics, Chemistry, and Computer Science at the University of Washington in Seattle, USA, researching object pose estimation visuomotor control and using molecular dynamics simulations to study the structure of cell membranes.

Luigi Gresele is a postdoctoral researcher at the University of Copenhagen. His research focuses on the intersection of representation learning and causal inference. Luigi earned his PhD from the Max Planck Institute for Intelligent Systems and the University of Tübingen, under the supervision of Bernhard Schölkopf. During his PhD, he took part in an ELLIS exchange with the Parietal team at Inria-CEA, hosted by Bertrand Thirion and Aapo Hyvärinen, and worked as an Applied Science Intern at Amazon Research Tübingen with Dominik Janzing. He co-organised the 2022 Workshop on Causal Representation Learning (CRL) at UAI Eindhoven, and the 2023 CIFAR and ELLIS Workshop on CRL in T\"ubingen.


\href{https://andreinicolicioiu.github.io/}{Andrei Nicolicioiu}
%Andrei Nicolicioiu
is a PhD student at Mila and Université de Montréal, Canada supervised by Aaron Courville with a Bachelor’s and a Master’s degree from the University Politehnica of Bucharest, Romania. Previously he worked as an ML researcher for 5 years at Bitdefender, Romania, and spent some time at the Max Planck Institute for Intelligent Systems, Germany. His research involves vision-language models with a focus on visual relational reasoning, out-of-distribution generalization and causally inspired representation learning, and data-selection methods.

Xiusi Li is a Master's student at Mila and McGill University supervised by Siamak Ravankbash. She has previously completed her integrated master's degree in Mathematics at the University of Oxford and worked as an ML engineer for 2 years at a biomedical startup in London. Her research interests include causal representation learning and more generally the use of symmetry in machine learning.

Sophie Xhonneux is a Ph.D.\ student at Mila and Universit\'{e} de Montr\'{e}al, Canada supervised by Gauthier Gidel and Jian Tang. She completed her a Bachelor's and a Master's degree in Computer Science at the University of Cambridge. Her research interest include robust and aligned large language models, graph representation learning, reasoning, and out-of-distribution generalisation.


Patrik Reizinger is a Ph.D.\ student at the Max Planck Institute for Intelligent Systems in Germany, supervised by Wieland Brendel, Ferenc Huszár, Matthias Bethge, and Bernhard Schölkopf. He is part of the ELLIS and IMPRS-IS programs. Before his Ph.D., he got a Bachelor's and Master's degree in electrical engineering from the Budapest University of Technology, where he specialized in control engineering and intelligent systems. His main research interests include causal representation learning and identifiability, which he also applies to understand emergent behavior in LLMs. He is currently visiting the Vector Institute in Toronto, Canada, hosted by Rahul G.~Krishnan.


[program committee and discussion of reviewing process]

% [number of organizers] dedicated specialists (deep bench of experience); we are bringing two different communities together. organizers need to cover not just one but two communities (and everything in between); knowledge transfer from experienced organizers; larger pool of reviewers to pull from (more institutions/groups involved)
    Our workshop brings together two communities, which requires experts from both as organizers. This enables us to have access to a larger and more diverse set of reviewers. Furthermore, our organizing team facilitates knowledge transfer regarding the best practices of organizing workshops, as it includes several junior people for whom this is the first experience as an organizer. 

\section{Selected References}

\bibliographystyle{unsrtnat}
\bibliography{refs}

\nocite{liuLargeLanguageModels2024}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \newpage

% \appendix

% \section{Appendix / supplemental material}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}