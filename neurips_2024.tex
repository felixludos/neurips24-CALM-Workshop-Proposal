\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
% \usepackage{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
% \usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
   \usepackage[nonatbib,preprint]{neurips_2024}

\usepackage[numbers]{natbib}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{emoji}
\usepackage[shortlabels]{enumitem}
\usepackage[para]{footmisc}
\setlist[itemize]{leftmargin=2em}
\setlist[enumerate]{leftmargin=2em}


\newcommand{\todo}[1]{\textcolor{red}{~TODO: #1}}


\title{Proposal for a Workshop on \\
Causality and Large Models (C\emoji{heart}LM)}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
    Felix Leeb \thanks{Correspondence: \texttt{calmworkshop2024@gmail.com}} 
    \thanks{Max Planck Institute for Intelligent Systems, Germany}
    \And
    Ching Lam Choi \footnotemark[1] \thanks{MIT, USA}
    \And
    Luigi Gresele \thanks{University of Copenhagen, Denmark}
    \And
    Josef Valvoda \footnotemark[4]
    \And
    Andrei Nicolicioiu \thanks{Mila, Canada}
    \And
    Xiusi Li \footnotemark[5]
    \And
    Patrik Reizinger \footnotemark[2]
    \And
    Sophie Xhonneux \footnotemark[5]
    \And
    Haoxuan Li \thanks{Peking University, China}
    \And
    Mengyue Yang \thanks{University College London, UK}
    \And
    Bernhard Schölkopf \footnotemark[2]
    \And
    Dhanya Sridhar \footnotemark[5]
}


\begin{document}


\maketitle


\begin{abstract}
[todo - based on topic section] emphasize this is not just language models
\url{https://calm-workshop-2024.github.io/}
\end{abstract}


% https://neurips.cc/Conferences/2024/WorkshopsGuidance

\section{Topic}

The remarkable capabilities and accessibility of recent large models, also known as ``foundation models,'' have sparked significant interest and excitement in the research community and beyond. In particular, large pre-trained generative models have demonstrated remarkable competencies in understanding and generating human-like text despite being trained on largely unstructured data using relatively simple self-supervised learning objectives. This raises the question: (A) \textit{Why do such large models work so well?} % for, in
% instruction following, agentic behaviors

The impressive performance, sometimes even exceeding human experts, across a wide variety of benchmarks, together with the incorporation of multiple modalities such as images, text, and audio, makes these large models particularly versatile decision-making systems.
However, the increased adoption of these models is not without challenges. The increasing size and complexity of these ``black box'' models raises concerns about their trustworthiness and reliability.
For real-world applications, where distribution shifts are pervasive and sufficient high-quality data may be difficult or expensive to collect, it is crucial to systematically verify and enhance the robustness and generalization capabilities of these models.   
This is especially pertinent in safety-critical domains, such as healthcare and policy-making. %, where decisions have significant real-world impact.
% it is essential to carefully ascertain the limitations of these models as well as systematically improve their robustness and interpretability. %, and reliability.
% \todo{The only prudent way for deploying large models in safety-critical scenarios is to understand..}
Consequently, we must consider: (B) \textit{Under what circumstances can we trust these large models and how can this be improved?} %and (C) \textit{How can we improve the interpretability and reliability of current models?}
% with, of
% \todo{highlight from the begining that large models are still not rubust enought, especially for distribution shifts. This is the main goal of the first 2 direction from bellow, and it will be good to introduce some relevant questions before. we could point to the fact that our theoretical understanding concerns mostly iid settings - and causality could be a good tool to move to OOD regimes.}
% (D) \textit{How to make large models more robust?}

Enter causality: a systematic framework to formalize ``why?'' and ``how?'' questions much like (A) or (B) and develop principled tools to address them.
Causal inference is a powerful approach to describe a system's behavior under interventions and reason over counterfactual scenarios. 
% Causal inference offers a systematic approach to understanding the causal relationships among the components of a system, which is crucial for 
By overcoming spurious correlations and statistical biases, and instead relying on predictive causal relationships, causal models can faithfully explain a system's behavior and enable performance guarantees beyond the training distribution, which is crucial for high-risk applications.
% by elucidating the causal relationships among its components. 
% This opens up substantial potential for using causality to address key open questions about large models, such as (A) or (B). 
% By leveraging causal inference, we hope to tackle these questions rigorously and enhance our understanding of these powerful models, as well as their reliability and trustworthiness.
% avoiding spurious correlations and non-causal biases in the data.
% In high-risk applications, where performance guarantees beyond the training distribution are desirable, causal inference is critical.
% Causal inference is crucial for systematically identifying
% Causal models explain a system's behavior by elucidating the causal relationships among its components.
However, translating the rigorous theoretical tools of causality into practical methods, especially in the large-scale regime with heterogeneous unstructured data as in large models, remains a notable challenge, despite the growing attention by the community.
%, especially in high-risk applications such as medical diagnosis or autonomous driving.

% This opens up substantial potential for using causality to address key open questions about large models. By leveraging causal inference, we hope to tackle these questions rigorously and enhance our understanding of these powerful models, as well as their reliability and trustworthiness.

% \todo{highlight from the begining that large models are still not rubust enought, especially for distribution shifts. This is the main goal of the first 2 direction from bellow, and it will be good to introduce some relevant questions before. we could point to the fact that our theoretical understanding concerns mostly iid settings - and causality could be a good tool to move to OOD regimes.}\todo{Improving the reasoning abilities and reliabilities of the current model. The logit is Why(A) -> What(B) -> How(C), before (C), should we take some examples to make (C) more concrete like: Therefore, it is essential to consider and explore how to systematically improve the reasoning capabilities and reliability of large models from multiple perspectives, including model architecture, algorithm improvements, data diversity, evaluation systems, and transparency.}
% % (D) \textit{How to make large models more robust?}

% Enter causality: a principled framework for predicting a system's behavior under interventions and reasoning over counterfactual scenarios. \todo{Should we emphasise that:  
% Causality is a very powerful framework that can help us analyze, evaluate, and enhance the trustworthiness of systems.}
% In high-risk applications, where performance guarantees beyond the training distribution are desirable, causal inference is critical.
% Moreover, causal models explain a system's behavior by elucidating the causal relationships among its components.
% This opens up substantial potential for using causality to address key questions about large models, such as (A), (B), or (C). By leveraging causal inference, we hope to tackle these questions rigorously and enhance our understanding of these powerful models, as well as their reliability and trustworthiness.

\iffalse
% rigorous
Enter causality. Causal inference offers a framework to articulate questions like (A), (B), or (C), and provides tools to answer them in a systematic way. 
{\color{blue}[We believe that this has ... potential for ... addressing critical questions on large models.]}
It provides a principled framework for predicting a system's behavior under interventions and reasoning over counterfactual scenarios.
For example, by systematically identifying failure modes 
we can gain insights into the causal factors that govern the model's behavior.
This can be crucial 
{\color{blue}[aims to predict a system's behavior under manipulations or interventions, or in ... counterfactual scenarios ... interpretable and reliable ...]
On the one hand, [this is desirable in ... high-risk applications such as medical ...]. On the other hand, [it ..., robustness and trustworthiness guarantees].
]}
\fi
% For example, by systematically identifying failure modes 
% or by isolating the model components that cause problematic behavior 
% we can gain insights into the causal factors that affect the model's behavior. 
% This can lead to more interpretable and explainable models, while also promising to improve model out-of-distribution generalization and robustness by avoiding spurious correlations and non-causal biases in the data.

% Our workshop will explore the many exciting synergies between causality and large models. 

With the striking potential of causality and the enormous interest in tackling the many open questions about understanding and improving large models on the other, we propose a workshop that aims to explore the many exciting synergies between causality and large models.
Specifically, we identify four main directions to cover in our workshop: 

% (1) Causality \textit{in} large models, (2) Causality \textit{for} large models, (3) Causality \textit{with} large models, and (4) Causality \textit{of} large models.

% In - Studying the causal knowledge captured by large models
%   - Evaluating the causal knowledge that emerges out of standard training schemes
%   - Do LMs do “amortized causality” (Bernhard’s term)?
%   - Related to: Psychometrics, behavioral sciences
%   - References: causal parrots, CLadder, Causal Reasoning in LLMs (2305.00050)
%   - Example application: holistic benchmarking
%   - Example approach: evaluate model biases with a synthetic benchmark
% - Of - Understanding the causal structure of how large models work
%   - Focus is on exploiting the structure of large models to make them more interpretable/controllable
%   - Related to: Mechanistic interpretability, alignment, neuroscience
%   - Example application: explainable/trustworthy AI
%   - Example approach: Identify what components of the model cause problematic behavior
% - For - Applying causal methods to improve large models
%   - ML, robustness, generalization, transfer methods
%   - Explicit or (often) implicit causal assumptions about the model/task
%   - “Climbing up the ladder” to improve our models
%   - Example approach: collecting interventional training data
%   - Example application: debiasing models
% - With - Leveraging large models to improve causal inference/discovery
%   - Core contribution is to causal inference/discovery
%   - How can the high performance of large models be exploited by causal methods?
%   - References: Can LLMs Build Causal Graphs (2303.05279), …
%   - Example application: causal discovery using LLMs
%   - Example approach: leveraging semantics learned in pre-training

\begin{enumerate}[nolistsep]
    \item \textbf{Causality \textit{in} large models}: Assessing the causal knowledge captured by large models and their (causal) reasoning abilities.
    % A growing area of research in this direction is to investigate the causal knowledge captured in large models despite being trained on unstructured, observational data.
    % amortized causal inference - causal knowledge *emerges* out of large-scale observational data 
    % how multiple modalities enable more causal reasoning (?)
    \item \textbf{Causality \textit{for} large models}: Applying ideas from causality to augment and improve large models.
    % Building off of efforts towards causal representation learning, we aim to share ideas on inductive biases inspired/grounded in causality for the large-scale regime
    \item \textbf{Causality \textit{with} large models}: Leveraging large models to improve causal inference and discovery.
    % is that recent large models appear to be able to perform some causal reasoning and extract some causal knowledge without deliberate or intended incorporation of causality in their conception.
    \item \textbf{Causality \textit{of} large models}: Investigating the causal structure of how large models work and how to make them more interpretable and controllable. 
    % Exploiting the rich framework of causal inference to understand the impressive capabilities of large models
\end{enumerate}

By engaging both theoretical and applied perspectives, we aim to foster deep insights into the behavior of large models, not only to assess their current capabilities but also to improve their performance and to make their behavior more interpretable and reliable. In addition to the directions discussed above, we welcome contributions that explore the vibrant interface between causality and large models, for example:

\begin{itemize}[nolistsep]
    \item Theoretical and empirical studies on how large models use causal knowledge for reasoning
    \item Applications of large models to real-world problems leveraging causal methods for improved performance or interpretability 
    \item Links between multiple modalities (such as images, text, and audio) and (causal) reasoning abilities of large models
    \item Causal methods that are readily applicable and useful to understanding or improving large models
\end{itemize}

% [specific criteria (for call for papers)]


\subsection{Speakers and Schedule}

Our workshop will feature a mix of invited talks, a poster session, a panel discussion, and interactive elements to engage the audience and foster discussion. 
We plan to hold a series of six invited talks given by both up-and-coming as well as distinguished researchers with experience in the intersection of causality and large models, as well as a panel discussion for deeper dialogue on the workshop's main themes. Overall, we estimate that our workshop will run for a full day (approximately 8-9 hours), with a preparatory rough breakdown as follows:


\begin{itemize}[nolistsep]
    \item 30 min: Opening and closing remarks
    \item 6 x 30 min: Invited talks
    \item 60 min: Panel discussion
    \item Up to 60 min: Contributed talks
    \item 60 min: Poster session (with up to an additional 60 min during coffee breaks)
    \item 90 min: Lunch break (with a voluntary ``lunch buddy'' program to connect participants)
    \item 2 x 30 min: Coffee breaks
\end{itemize}



% [lunch buddy program] to connect not just junior+senior people for lunch but also connecting different communities
    % To connect junior and senior researchers and simultaneously encourage cooperation between the causality and large model communities, we---inspired by the success of a similar initiative at the NeurIPS workshop NeurReps---plan to setup a ``lunch buddy" system between the participants, making our workshop more interactive.


Taking inspiration from a similar idea at the NeurIPS 2022 workshop NeurReps and to foster a more interactive and engaging environment, we plan to include a ``lunch buddy'' program to connect junior and senior researchers, as well as researchers from different communities, for lunch. 
Attendees as well as participants will be able to sign up for this program throughout the morning of the workshop before lunch, self-reporting their research interests and experience level, then we will randomly pair them up and provide them with a list of suggested topics to discuss during lunch.
This will provide an opportunity for participants to network and engage in discussions with researchers that have different backgrounds, but similar research interests.

% Our workshop will feature a mix of talks, panel discussions, and interactive elements to engage the audience and foster discussion. We plan to have a total of 6 talks, 2 panel discussions, and 1 interactive session. The schedule will be structured to allow for ample time for questions and discussion, as well as breaks to encourage networking and collaboration among the participants.


% We also selected several notable researchers that focus on adjacent or applied fields to provide our attendees with insights in how state-of-the-art methods can rapidly and effectively be deployed to solve problems.
% features a distinguished group of speakers and panelists. 

% We have invited {\color{blue} [potentially turn this into a bulletpoint list \& add affiliation/position? It would showcase the diversity in seniority \& institutions]}  
% {\color{green} [link speaker's website and short description?]}

\paragraph{Speakers and Panelists}

All our invited speakers and panelists have confirmed their participation except one~\footnote{Matej Zečević, who is still pending confirmation}.
We are very encouraged by the enthusiastic reception our proposed workshop has already received from the speakers and panelists we invited, with several speakers expressing their excitement at the topic and the opportunity to engage with the community.

% We are also in the process of finalizing the schedule, which will feature a mix of talks, panel discussions, and interactive elements to engage the audience and foster discussion.

% a great lineup of speakers and panellists from a wide seniority range, including academic and industry experts.

Our confirmed\footnote{All our speakers and panellists have confirmed their participation, except Matej Zečević.} \textit{speakers} are: 
\href{https://www.janexwang.com/}{Jane X. Wang} (staff research scientist, DeepMind),
\href{https://www.claudiajshi.com/}{Claudia Shi} (Ph.D.\ student, Columbia University),
\href{http://victorveitch.com/}{Victor Veitch} (Assistant professor, University of Chicago; research scientist, Google Brain),
\href{https://amitsharma.in/}{Amit Sharma} (Principal researcher, Microsoft Research),
\href{https://causalai.net/}{Elias Bareinboim} (Associate professor, Columbia University), 
\href{https://www.matej-zecevic.de/}{Matej Zečević} (Ph.D.\ student, TU Darmstadt; to be confirmed)

Our confirmed \textit{panellists} are:
\href{https://maria-antoniak.github.io/}{Maria Antoniak} (Young Investigator, Allen Institute for AI),
\href{https://sites.google.com/view/giambattista-parascandolo/home}{Giambattista Parascandolo} (Research scientist, OpenAI; tentatively accepted),
\href{https://atticusg.github.io/}{Atticus Geiger} (Principal investigator, Stanford University),
\href{https://www.bunnelab.com/}{Charlotte Bunne} (Assistant professor, EPFL),
\href{https://ai.stanford.edu/~cbfinn/}{Chelsea Finn} (Assistant professor, Stanford University) , and
\href{https://zhijing-jin.com/}{Zhijing Jin} (Ph.D.\ student, MPI-IS and ETH Zürich).


As our workshop endeavors to bring together two research sub-communities large-scale machine learning and causal inference, we are fortunate to draw our speakers and panelists from diverse backgrounds and institutions.
We are excited to have a mix of senior and junior researchers, as well as a blend of academic and industry experts, to provide a multi-faceted perspective on causality and large models.
Additionally, to encourage a more inclusive and diverse environment, we have also selected several notable researchers who focus on adjacent or applied fields to provide our attendees with insights into how state-of-the-art methods can rapidly and effectively be deployed to solve real-world problems.


\paragraph*{Program Committee}

We are also happy to announce that, we have already confirmed 34 reviewers for our workshop, based on a voluntary form sent out internally at two of the six institutions represented by our organizing committee. Although this is already a substantial number of reviewers, at this early stage, we will continue to reach out to nominate additional qualified reviewers 
to ensure a fair and thorough reviewing process.
Several of the organizers will serve as area chairs to oversee the reviewing process.

% For our program committee, we have sent out invitations internally at our institutions and to our collaborators, and we are in the process of finalizing the committee. We aim to have a diverse and inclusive program committee to ensure a fair and thorough reviewing process.

% [speakers represent industry perspective]
% {\color{cyan} see above}

% [note on diversity]
% {\color{cyan} see above}

% \subsection{Interactive Elements}

% [equipment needs]


\subsection{Relevance and Impact}

Although there have been several past workshops focusing on various aspects of large models in NeurIPS and other major conferences, to our knowledge, there has not been a workshop that specifically focuses on the intersection between causality and large models.

There has been a growing interest in making causality more accessible to the machine learning community, as evidenced by the several past Causal Representation Learning workshops. We hope to carry on this mission by discussing how a causal framing can enable more principled approaches and potentially even new capabilities such as systematic generalization or robustness.

[high application and industry cares a lot about tthis]

[comparison to CRL] include stats on CRL workshops

% why now? (why is this workshop timely?)

We hope to focus on the intersection between causality and large models in particular, as the paradigm shift towards large models has brought new challenges and opportunities that are rife for deeper discussion.

% Past related workshops, such as the Causal Representation Learning workshop at NeurIPS 2023, have strived to make causal methods more accessible to the machine learning community, and we hope to carry on this mission by discussing how a causal framing can enable more principled approaches and potentially even new capabilities such as systematic generalization or robustness. However, 
% For example, how this knowledge is utilized for "amortized causal reasoning" for solving complex tasks.

% crystallize emerging subfields
% 

% [list of "selected references"]

% C♥LM is timely amidst the safety and robustness concerns of AI in high-stakes settings, and that causality is an important angle to consider

% how causal understanding is fundamental for achieving robust performance in multi-modal, healthcare settings.


\section{Team}

[introduction and affiliations]

Felix Leeb [MPI], Ching Lam Choi [MIT], Luigi Gresele [KU], Josef Valvoda [KU],  Andrei Nicolicioiu [Mila], Xiusi Li [Mila], Patrik Reizinger [MPI], Sophie Xhonneux [Mila],
Haoxuan Li [Peking University], Mengyue Yang [UCL]
PIs (2): Bernhard Schölkopf [MPI], Dhanya Sridhar [Mila]

Bio:\\

\href{https://felixludos.com/}{Felix Leeb} 
is a senior Ph.D.\ student at the Max Planck Institute for Intelligent Systems in Germany, under the supervision of Bernhard Schölkopf working on causal representation learning and evaluating the reasoning abilities of large language models. Before living in Germany, Felix completed his Bachelors in Physics, Chemistry, and Computer Science at the University of Washington in Seattle, USA, researching object pose estimation visuomotor control and using molecular dynamics simulations to study the structure of cell membranes.

\href{https://chinglamchoi.github.io/cchoi/}{Ching Lam Choi} is an incoming Ph.D.\ student at MIT EECS; she has also spent time with brilliant mentors and peers at Mila, Max Planck Institute for Intelligent Systems, University of Copenhagen, CUHK MMLab and more. Her experiences inspire her to advocate for diverse research participation, via hosting open office hours and co-organising ``New in ML Workshop" at NeurIPS 2023, ``CoSubmitting Summer Workshop" at ICLR 2022, and ``Undergraduates in Computer Vision Social" in ICCV 2021. Her research interests are in responsibly robust scaling, through better synthetic data and causal reasoning capabilities.

\href{https://lgresele.github.io/}{Luigi Gresele}  is a postdoctoral researcher at the University of Copenhagen. His research focuses on the intersection of representation learning and causal inference. Luigi earned his Ph.D.\ from the Max Planck Institute for Intelligent Systems and the University of Tübingen, under the supervision of Bernhard Schölkopf. During his Ph.D.\, he took part in an ELLIS exchange with the Parietal team at Inria-CEA, hosted by Bertrand Thirion and Aapo Hyvärinen, and worked as an Applied Science Intern at Amazon Research Tübingen with Dominik Janzing. He co-organised the 2022 Workshop on Causal Representation Learning (CRL) at UAI Eindhoven, and the 2023 CIFAR and ELLIS Workshop on CRL in T\"ubingen.


\href{https://andreinicolicioiu.github.io/}{Andrei Nicolicioiu}
%Andrei Nicolicioiu
is a Ph.D.\ student at Mila and Universit\'{e} de Montr\'{e}al, Canada supervised by Aaron Courville with a Bachelor’s and a Master’s degree from the University Politehnica of Bucharest, Romania. Previously he worked as an ML researcher for 5 years at Bitdefender, Romania, and spent some time at the Max Planck Institute for Intelligent Systems, Germany. His research involves vision-language models with a focus on visual relational reasoning, out-of-distribution generalization and causally inspired representation learning, and data-selection methods.

Xiusi Li is a Master's student at Mila and McGill University supervised by Siamak Ravankbash. She has previously completed her integrated master's degree in Mathematics at the University of Oxford and worked as an ML engineer for 2 years at a biomedical startup in London. Her research interests include causal representation learning and more generally the use of symmetry in machine learning.

Sophie Xhonneux is a Ph.D.\ student at Mila and Universit\'{e} de Montr\'{e}al, Canada supervised by Gauthier Gidel and Jian Tang. She completed her a Bachelor's and a Master's degree in Computer Science at the University of Cambridge. Her research interest include robust and aligned large language models, graph representation learning, reasoning, and out-of-distribution generalisation.


\href{https://rpatrik96.github.io/}{Patrik Reizinger} is a Ph.D.\ student at the Max Planck Institute for Intelligent Systems in Germany, supervised by Wieland Brendel, Ferenc Huszár, Matthias Bethge, and Bernhard Schölkopf. He is part of the ELLIS and IMPRS-IS programs. Before his Ph.D., he got a Bachelor's and Master's degree in electrical engineering from the Budapest University of Technology, where he specialized in control engineering and intelligent systems. His main research interests include causal representation learning and identifiability, which he also applies to understand emergent behavior in LLMs. He is currently visiting the Vector Institute in Toronto, Canada, hosted by Rahul G.~Krishnan.

\href{https://valvoda.github.io/}{Josef Valvoda} is a postdoctoral researcher at the University of Copenhagen. 
Before starting his postdoc, Josef obtained his Ph.D. from the University of Cambridge, where he was co-advised by Ryan Cotterell (ETH) and Simone Teufel (Cambridge).
His work is focused on formalizing legal reasoning---he has completed his undergraduate degree in Law---as well as the study of neural models through formal language theory.
He has previously interned as an Applied Scientist at Amazon, ML/AI researcher at Apple and will be joining Google X for a residency this summer.

\href{https://scholar.google.com/citations?user=gtDqiucAAAAJ&hl=en}{Haoxuan Li} is a Ph.D. student in Center for Data Science, Peking University. His research interests span from causal machine learning theory, counterfactual fairness, recommender system debiasing, out-of-distribution generalization, multi-source data fusion, bioinformatics, and large language models. He has published more than 20 papers as first author in top conference
proceedings, including ICML, NeurIPS, ICLR, SIGKDD, WWW, SIGIR, ICDE, AAAI, and IJCAI, etc., and recieved the NSFC Young Scientists Fund (2024). Moreover, he has been served as the PC member (or area chair) for several top conferences including ICML, NeurIPS, ICLR, SIGKDD, WWW, AAAI, IJCAI, and MM, also the invited reviewer for prestigious journals such as The Innovation, TOIS, TKDE, TKDD, TNNLS, and IPM.

\href{https://ymy4323460.github.io/}{Mengyue Yang} is a doctoral student from University College London, supervised by Professor Jun Wang. Her research interests lie in causality, multi-agent systems, reinforcement learning and human-AI coordination. She was co-organizer of the causal representation learning workshop at ICDM 2024 and Competition Causal Structure Learning from Event Sequences and Prior Knowledge at NeruIPS 2023.

[specialized teams and responsibilities]

[program committee and discussion of reviewing process]

% [number of organizers] dedicated specialists (deep bench of experience); we are bringing two different communities together. organizers need to cover not just one but two communities (and everything in between); knowledge transfer from experienced organizers; larger pool of reviewers to pull from (more institutions/groups involved)
    Our workshop brings together two communities, which requires experts from both as organizers. This enables us to have access to a larger and more diverse set of reviewers. Furthermore, our organizing team facilitates knowledge transfer regarding the best practices of organizing workshops, as it includes several junior people for whom this is the first experience as an organizer. 

\section{Selected References}

{\color{blue}[Whenever possible, I would suggest adding at least one reference for each of the invited speakers/panelists/organisers, to showcase that each is a good fit]}

\bibliographystyle{unsrtnat}
\renewcommand{\bibsection}{}
\bibliography{refs}

\nocite{liuLargeLanguageModels2024, anwar2024foundational, jin2024cladder, kasetty2024evaluating,park2023linear,zevcevic2023causal,vashishtha2023causal, reizinger2024understanding}
\nocite{gupta2024context_is_env, lampinen2023passive}
\nocite{kiciman2023causal}
\nocite{daunhawer2023identifiability}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \newpage

% \appendix

% \section{Appendix / supplemental material}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}