\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
% \usepackage{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
% \usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
   \usepackage[nonatbib,preprint]{neurips_2024}

\usepackage[numbers]{natbib}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{emoji}
\usepackage[shortlabels]{enumitem}
\setlist[itemize]{leftmargin=2em}
\setlist[enumerate]{leftmargin=2em}


\title{Proposal for a Workshop on \\
Causality and Large Models (C\emoji{heart}LM)}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
    Felix Leeb \thanks{Correspondence: \texttt{calmworkshop2024@gmail.com}} \thanks{Max Planck Institute for Intelligent Systems}
    \And
    Ching Lam Choi \footnotemark[1] \thanks{MIT}
    \And
    Luigi Gresele \thanks{University of Copenhagen}
    \And
    Josef Valvoda \footnotemark[4]
    \And
    Andrei Nicolicioiu \thanks{Mila}
    \And
    Xiusi Li \footnotemark[5]
    \And
    Patrik Reizinger \footnotemark[2]
    \And
    Sophie Xhonneux \footnotemark[5]
    \And
    Bernhard Schölkopf \footnotemark[2]
    \And
    Dhanya Sridhar \footnotemark[5]
    % \And % (waiting for response)
    % Nichlas Papernot
}


\begin{document}


\maketitle


\begin{abstract}
[todo - based on topic section] emphasize this is not just language models
\url{https://imur-workshop-test.github.io/}
\end{abstract}


% https://neurips.cc/Conferences/2024/WorkshopsGuidance

\section{Topic}

The remarkable capabilities and accessibility of recent large models, also known as ``foundation models,'' have sparked significant interest and excitement in the research community and beyond. In particular, large pre-trained generative models have demonstrated remarkable competencies in understanding and generating human-like text, images, and more despite being trained on largely unstructured data using relatively simple self-supervised learning objectives. This raises the question: (A) \textit{Why do such large models work so well?} % for, in
% instruction following, agentic behaviors

The impressive performance, sometimes even exceeding human experts, across a wide variety of benchmarks, together with the incorporation of multiple modalities such as images, text, audio, makes it tempting to regard these large models as versatile decision-making systems.
However, the increased adoption of these models is not without challenges. The increasing size and complexity of these ``black box'' models raises concerns about their trustworthiness and reliability.
This is especially pertinent in high-stakes domains, such as healthcare and policy-making, where decisions have significant real-world impact.
Consequently, we must consider: (B) \textit{Under what circumstances can we trust these large models?} and (C) \textit{How can we improve the reliability and trustworthiness of current models?}
% with, of

(D) \textit{How to make large models more robust?}

Enter causality, a principled framework for predicting a system's behavior under interventions and reasoning over counterfactual scenarios.
In high-risk applications, where performance guarantees beyond the training distribution are desirable, causal inference is critical.
Moreover, causal models explain a system's behavior by elucidating the causal relationships among its components.
This opens up substantial potential for using causality to address key questions about large models, such as (A), (B), or (C). By leveraging causal inference, we hope to tackle these questions rigorously and enhance our understanding of these powerful models, as well as their reliability and trustworthiness.

\iffalse
% rigorous
Enter causality. Causal inference offers a framework to articulate questions like (A), (B), or (C), and provides tools to answer them in a systematic way. 
{\color{blue}[We believe that this has ... potential for ... addressing critical questions on large models.]}
It provides a principled framework for predicting a system's behavior under interventions and reasoning over counterfactual scenarios.
For example, by systematically identifying failure modes 
we can gain insights into the causal factors that govern the model's behavior.
This can be crucial 
{\color{blue}[aims to predict a system's behavior under manipulations or interventions, or in ... counterfactual scenarios ... interpretable and reliable ...]
On the one hand, [this is desirable in ... high-risk applications such as medical ...]. On the other hand, [it ..., robustness and trustworthiness guarantees].
]}
\fi
% For example, by systematically identifying failure modes 
% or by isolating the model components that cause problematic behavior 
% we can gain insights into the causal factors that affect the model's behavior. 
% This can lead to more interpretable and explainable models, while also promising to improve model out-of-distribution generalization and robustness by avoiding spurious correlations and non-causal biases in the data.

% {\color{blue}[Work in progress --- alternative proposal for the paragraph above:

% Causality provides a framework through which questions such as (A), (B) and (C) may be addressed. 
% }

Our workshop will explore the many exciting synergies between causality and large models. 
Specifically, we identify four main directions to cover in our workshop: 
% (1) Causality \textit{in} large models, (2) Causality \textit{for} large models, (3) Causality \textit{with} large models, and (4) Causality \textit{of} large models.

% In - Studying the causal knowledge captured by large models
%   - Evaluating the causal knowledge that emerges out of standard training schemes
%   - Do LMs do “amortized causality” (Bernhard’s term)?
%   - Related to: Psychometrics, behavioral sciences
%   - References: causal parrots, CLadder, Causal Reasoning in LLMs (2305.00050)
%   - Example application: holistic benchmarking
%   - Example approach: evaluate model biases with a synthetic benchmark
% - Of - Understanding the causal structure of how large models work
%   - Focus is on exploiting the structure of large models to make them more interpretable/controllable
%   - Related to: Mechanistic interpretability, alignment, neuroscience
%   - Example application: explainable/trustworthy AI
%   - Example approach: Identify what components of the model cause problematic behavior
% - For - Applying causal methods to improve large models
%   - ML, robustness, generalization, transfer methods
%   - Explicit or (often) implicit causal assumptions about the model/task
%   - “Climbing up the ladder” to improve our models
%   - Example approach: collecting interventional training data
%   - Example application: debiasing models
% - With - Leveraging large models to improve causal inference/discovery
%   - Core contribution is to causal inference/discovery
%   - How can the high performance of large models be exploited by causal methods?
%   - References: Can LLMs Build Causal Graphs (2303.05279), …
%   - Example application: causal discovery using LLMs
%   - Example approach: leveraging semantics learned in pre-training

\begin{enumerate}[nolistsep]
    \item \textbf{Causality \textit{in} large models}: Assessing the causal knowledge captured by large models and their causal reasoning abilities. 
    % A growing area of research in this direction is to investigate the causal knowledge captured in large models despite being trained on unstructured, observational data.
    % amortized causal inference - causal knowledge *emerges* out of large-scale observational data 
    % how multiple modalities enable more causal reasoning (?)
    \item \textbf{Causality \textit{for} large models}: Applying ideas from causality to augment and improve large models.
    % Building off of efforts towards causal representation learning, we aim to share ideas on inductive biases inspired/grounded in causality for the large-scale regime
    \item \textbf{Causality \textit{with} large models}: Leveraging large models to improve causal inference and discovery.
    % is that recent large models appear to be able to perform some causal reasoning and extract some causal knowledge without deliberate or intended incorporation of causality in their conception.
    \item \textbf{Causality \textit{of} large models}: Exploiting the rich framework of causal inference %can be used 
    to understand and interpret the impressive capabilities of large models. 

\end{enumerate}

By engaging both theoretical and applied perspectives, we aim to foster deep insights into the behavior of large models, not only to assess their current capabilities, but also to improve their performance and to make their behavior more interpretable and reliable.

% [specific criteria (for call for papers)]


\subsection{Speakers and Schedule}


We plan to hold a series of talks given by both up-and-coming as well as distinguished researchers with experience

We are fortunate to draw our speakers and panelists not just from one, but from two thriving and diverse research communities: large-scale machine learning and causal inference.

We also selected several notable researchers that focus on adjacent or applied fields to provide our attendees with insights in how state-of-the-art methods can rapidly and effectively be deployed to solve problems.

features a distinguished group of speakers and panelists. 
We have invited {\color{blue} [potentially turn this into a bulletpoint list \& add affiliation/position? It would showcase the diversity in seniority \& institutions]}  
{\color{green} [link speaker's website and short description?]}
a great lineup of speakers and panellists from a wide seniority range, including academic and industry experts.
Our confirmed\footnote{All our speakers and panellists have confirmed their participation, except Matej Zečević.} \textit{speakers} are: \href{https://www.janexwang.com/}{Jane X. Wang} (staff research scientist, DeepMind ,
\href{https://www.claudiajshi.com/}{Claudia Shi} (Ph.D.\ student, Columbia University),
\href{http://victorveitch.com/}{Victor Veitch} (Assistant professor, University of Chicago; research scientist, Google Brain),
\href{https://amitsharma.in/}{Amit Sharma} (Principal researcher, Microsoft Research),
\href{https://causalai.net/}{Elias Bareinboim} (Associate professor, Columbia University), \href{https://www.matej-zecevic.de/}{Matej Zečević} (Ph.D.\ student, TU Darmstadt; to be confirmed)

Our confirmed \textit{panellists} are:
\href{https://maria-antoniak.github.io/}{Maria Antoniak} (Young Investigator, Allen Institute for AI),
\href{https://sites.google.com/view/giambattista-parascandolo/home}{Giambattista Parascandolo} (Research scientist, OpenAI; tentatively accepted),
\href{https://atticusg.github.io/}{Atticus Geiger} (Principal investigator, Stanford University),
\href{https://www.bunnelab.com/}{Charlotte Bunne} (Assistant professor, EPFL),
\href{https://ai.stanford.edu/~cbfinn/}{Chelsea Finn} (Assistant professor, Stanford University) , and
\href{https://zhijing-jin.com/}{Zhijing Jin} (Ph.D.\ student, MPI-IS and ETH Zürich).

[speakers represent industry perspective]
{\color{cyan} see above}

[note on diversity]
{\color{cyan} see above}

% \subsection{Interactive Elements}

% [equipment needs]

[lunch buddy program] to connect not just junior+senior people for lunch but also connecting different communities
    To connect junior and senior researchers and simultaneously encourage cooperation between the causality and large model communities, we---inspired by the success of a similar initiative at the NeurIPS workshop NeurReps---plan to setup a ``lunch buddy" system between the participants, making our workshop more interactive.


\subsection{Relevance and Impact}

Although there have been several past workshops focusing on various aspects of large models in NeurIPS and other major conferences, to our knowledge, there has not been a workshop that specifically focuses on the intersection between causality and large models.

There has been a growing interest in making causality more accessible to the machine learning community, as evidenced by the several past Causal Representation Learning workshops. We hope to carry on this mission by discussing how a causal framing can enable more principled approaches and potentially even new capabilities such as systematic generalization or robustness.

[high application and industry cares a lot about tthis]

[comparison to CRL] include stats on CRL workshops

% why now? (why is this workshop timely?)

We hope to focus on the intersection between causality and large models in particular, as the paradigm shift towards large models has brought new challenges and opportunities that are rife for deeper discussion.

% Past related workshops, such as the Causal Representation Learning workshop at NeurIPS 2023, have strived to make causal methods more accessible to the machine learning community, and we hope to carry on this mission by discussing how a causal framing can enable more principled approaches and potentially even new capabilities such as systematic generalization or robustness. However, 
% For example, how this knowledge is utilized for "amortized causal reasoning" for solving complex tasks.

% crystallize emerging subfields
% 

% [list of "selected references"]

% C♥LM is timely amidst the safety and robustness concerns of AI in high-stakes settings, and that causality is an important angle to consider

% how causal understanding is fundamental for achieving robust performance in multi-modal, healthcare settings.


\section{Team}

[introduction and affiliations]

Felix Leeb [MPI], Ching Lam Choi [MIT], Luigi Gresele [KU], Josef Valvoda [KU],  Andrei Nicolicioiu [Mila], Xiusi Li [Mila], Patrik Reizinger [MPI], Sophie Xhonneux [Mila],
Haoxuan Li [Peking University], Mengyue Yang [UCL]
PIs (2): Bernhard Schölkopf [MPI], Dhanya Sridhar [Mila]

Bio:\\

\href{https://felixludos.com/}{Felix Leeb} 
is a Ph.D.\ student at the Max Planck Institute for Intelligent Systems in Germany, under the supervision of Bernhard Schölkopf working on causal representation learning and evaluating the reasoning abilities of large language models. Before living in Germany, Felix completed his Bachelors in Physics, Chemistry, and Computer Science at the University of Washington in Seattle, USA, researching object pose estimation visuomotor control and using molecular dynamics simulations to study the structure of cell membranes.

Ching Lam Choi is an incoming Ph.D.\ student at MIT EECS; she has also spent time with brilliant mentors and peers at Mila, Max Planck Institute for Intelligent Systems, University of Copenhagen, CUHK MMLab and more. Her experiences inspire her to advocate for diverse research participation, via hosting open office hours and co-organising ``New in ML Workshop" at NeurIPS 2023, ``CoSubmitting Summer Workshop" at ICLR 2022, and ``Undergraduates in Computer Vision Social" in ICCV 2021. Her research interests are in responsibly robust scaling, through better synthetic data and causal reasoning capabilities.

\href{https://lgresele.github.io/}{Luigi Gresele}  is a postdoctoral researcher at the University of Copenhagen. His research focuses on the intersection of representation learning and causal inference. Luigi earned his Ph.D.\ from the Max Planck Institute for Intelligent Systems and the University of Tübingen, under the supervision of Bernhard Schölkopf. During his Ph.D.\, he took part in an ELLIS exchange with the Parietal team at Inria-CEA, hosted by Bertrand Thirion and Aapo Hyvärinen, and worked as an Applied Science Intern at Amazon Research Tübingen with Dominik Janzing. He co-organised the 2022 Workshop on Causal Representation Learning (CRL) at UAI Eindhoven, and the 2023 CIFAR and ELLIS Workshop on CRL in T\"ubingen.


\href{https://andreinicolicioiu.github.io/}{Andrei Nicolicioiu}
%Andrei Nicolicioiu
is a Ph.D.\ student at Mila and Université de Montréal, Canada supervised by Aaron Courville with a Bachelor’s and a Master’s degree from the University Politehnica of Bucharest, Romania. Previously he worked as an ML researcher for 5 years at Bitdefender, Romania, and spent some time at the Max Planck Institute for Intelligent Systems, Germany. His research involves vision-language models with a focus on visual relational reasoning, out-of-distribution generalization and causally inspired representation learning, and data-selection methods.

Xiusi Li is a Master's student at Mila and McGill University supervised by Siamak Ravankbash. She has previously completed her integrated master's degree in Mathematics at the University of Oxford and worked as an ML engineer for 2 years at a biomedical startup in London. Her research interests include causal representation learning and more generally the use of symmetry in machine learning.

Sophie Xhonneux is a Ph.D.\ student at Mila and Universit\'{e} de Montr\'{e}al, Canada supervised by Gauthier Gidel and Jian Tang. She completed her a Bachelor's and a Master's degree in Computer Science at the University of Cambridge. Her research interest include robust and aligned large language models, graph representation learning, reasoning, and out-of-distribution generalisation.


\href{https://rpatrik96.github.io/}{Patrik Reizinger} is a Ph.D.\ student at the Max Planck Institute for Intelligent Systems in Germany, supervised by Wieland Brendel, Ferenc Huszár, Matthias Bethge, and Bernhard Schölkopf. He is part of the ELLIS and IMPRS-IS programs. Before his Ph.D., he got a Bachelor's and Master's degree in electrical engineering from the Budapest University of Technology, where he specialized in control engineering and intelligent systems. His main research interests include causal representation learning and identifiability, which he also applies to understand emergent behavior in LLMs. He is currently visiting the Vector Institute in Toronto, Canada, hosted by Rahul G.~Krishnan.

\href{https://valvoda.github.io/}{Josef Valvoda} is a postdoctoral researcher at the University of Copenhagen. 
Before starting his postdoc, Josef obtained his Ph.D. from the University of Cambridge, where he was co-advised by Ryan Cotterell (ETH) and Simone Teufel (Cambridge).
His work is focused on formalizing legal reasoning---he has completed his undergraduate degree in Law---as well as the study of neural models through formal language theory.
He has previously interned as an Applied Scientist at Amazon, ML/AI researcher at Apple and will be joining Google X for a residency this summer.


[specialized teams and responsibilities]

[program committee and discussion of reviewing process]

% [number of organizers] dedicated specialists (deep bench of experience); we are bringing two different communities together. organizers need to cover not just one but two communities (and everything in between); knowledge transfer from experienced organizers; larger pool of reviewers to pull from (more institutions/groups involved)
    Our workshop brings together two communities, which requires experts from both as organizers. This enables us to have access to a larger and more diverse set of reviewers. Furthermore, our organizing team facilitates knowledge transfer regarding the best practices of organizing workshops, as it includes several junior people for whom this is the first experience as an organizer. 

\section{Selected References}

{\color{blue}[Whenever possible, I would suggest adding at least one reference for each of the invited speakers/panelists/organisers, to showcase that each is a good fit]}

\bibliographystyle{unsrtnat}
\bibliography{refs}

\nocite{liuLargeLanguageModels2024, anwar2024foundational, jin2024cladder, kasetty2024evaluating,park2023linear,zevcevic2023causal,vashishtha2023causal, reizinger2024understanding, gupta2024context_is_env, lampinen2023passive}

\nocite{daunhawer2023identifiability}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \newpage

% \appendix

% \section{Appendix / supplemental material}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}