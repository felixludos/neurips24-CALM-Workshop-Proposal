\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
% \usepackage{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
% \usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
   \usepackage[nonatbib,preprint]{neurips_2024}

\usepackage[numbers]{natbib}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{emoji}
\usepackage[shortlabels]{enumitem}
\usepackage[para]{footmisc}
\setlist[itemize]{leftmargin=2em}
\setlist[enumerate]{leftmargin=2em}


\newcommand{\todo}[1]{\textcolor{red}{~TODO: #1}}


\title{Proposal for a Workshop on \\
Causality and Large Models (C\emoji{heart}LM)}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
    Felix Leeb \mbox{\thanks{Correspondence: \texttt{calmworkshop2024@gmail.com}} 
    \thanks{Max Planck Institute for Intelligent Systems, Germany}}
    \And
    Ching Lam Choi  \mbox{\footnotemark[1] \thanks{MIT, USA}}
    \And
    Luigi Gresele \mbox{\thanks{University of Copenhagen, Denmark}}
    \And
    Josef Valvoda \mbox{\footnotemark[4]}
    \And
    Andrei Nicolicioiu \mbox{\thanks{Mila, Canada}}
    \And
    Xiusi Li \mbox{\footnotemark[5]}
    \And
    Patrik Reizinger \mbox{\footnotemark[2]}
    \And
    Sophie Xhonneux \mbox{\footnotemark[5]}
    \And
    Haoxuan Li \mbox{\thanks{Peking University, China}}
    \And
    Mengyue Yang \mbox{\thanks{University College London, UK}}
    \And
    Bernhard Schölkopf \mbox{\footnotemark[2]}
    \And
    Dhanya Sridhar \mbox{\footnotemark[5]}
}


\begin{document}


\maketitle


\begin{abstract}
Our workshop aims to explore the synergies between causality and large models, also known as ``foundation models,'' which have demonstrated remarkable capabilities across multiple modalities (text, images, audio, etc.). Despite their high performance, the opaque nature of these large models raises crucial questions regarding their trustworthiness, especially in safety-critical domains. 
A growing community of researchers is turning towards a more principled framework to address these concerns, better understand the behavior of large models, and improve their reliability: causality.
% Here to leverage causal inference as a framework to systematically address and enhance the understanding and reliability of these models. 
Specifically, this workshop will focus on four directions: causality \textit{in} large models, to assess their causal reasoning abilities, causality \textit{for} improving large models, causality \textit{with} large models to enhance causal inference and discovery methods, and causality \textit{of} large models to understand and control their internal mechanisms. 
The invited speakers and panelists (almost all of which have already been confirmed to attend) represent a diverse set of perspectives and expertise, across both academia and industry.
The workshop is organized by a team of 12 members from six different institutions across North America, Europe, and Asia, ensuring diversity across research interests, backgrounds, and demographics. Visit our website: \url{https://calm-workshop-2024.github.io/}

% [todo - based on topic section] emphasize this is not just language models
% \url{https://calm-workshop-2024.github.io/}
\end{abstract}


% https://neurips.cc/Conferences/2024/WorkshopsGuidance

\section{Topic}

The remarkable capabilities and accessibility of recent large models, also known as ``foundation models,'' have sparked significant interest and excitement in the research community and beyond. In particular, large pre-trained generative models have demonstrated remarkable competencies in understanding and generating human-like text despite being trained on largely unstructured data using relatively simple self-supervised learning objectives. This raises the question: (A) \textit{Why do such large models work so well?} % for, in
% instruction following, agentic behaviors

The impressive performance, sometimes even exceeding human experts, across a wide variety of benchmarks, together with the incorporation of multiple modalities such as images, text, and audio, makes these large models particularly versatile decision-making systems.
However, the increased adoption of these models is not without challenges. The increasing size and complexity of these ``black box'' models raises concerns about their trustworthiness and reliability.
For real-world applications, where distribution shifts are pervasive and sufficient high-quality data may be difficult or expensive to collect, it is crucial to systematically verify and enhance the robustness and generalization capabilities of these models.   
This is especially pertinent in safety-critical domains, such as healthcare and policy-making. %, where decisions have significant real-world impact.
% it is essential to carefully ascertain the limitations of these models as well as systematically improve their robustness and interpretability. %, and reliability.
% \todo{The only prudent way for deploying large models in safety-critical scenarios is to understand..}
Consequently, we must consider: (B) \textit{Under what circumstances can we trust these large models and how can this be improved?} %and (C) \textit{How can we improve the interpretability and reliability of current models?}
% with, of
% \todo{highlight from the begining that large models are still not rubust enought, especially for distribution shifts. This is the main goal of the first 2 direction from bellow, and it will be good to introduce some relevant questions before. we could point to the fact that our theoretical understanding concerns mostly iid settings - and causality could be a good tool to move to OOD regimes.}
% (D) \textit{How to make large models more robust?}

Enter causality: a systematic framework to formalize ``why?'' and ``how?'' questions much like (A) or (B) and develop principled tools to address them.
Causal inference is a powerful approach to describe a system's behavior under interventions and reason over counterfactual scenarios. 
% Causal inference offers a systematic approach to understanding the causal relationships among the components of a system, which is crucial for 
By %overcoming %and statistical biases, 
%and 
relying on stable causal relationships, 
instead of potentially spurious statistical correlations,
causal models can transparently elucidate a system's behavior and enable performance guarantees beyond the training distribution, which is crucial for high-risk applications.
% by elucidating the causal relationships among its components. 
% This opens up substantial potential for using causality to address key open questions about large models, such as (A) or (B). 
% By leveraging causal inference, we hope to tackle these questions rigorously and enhance our understanding of these powerful models, as well as their reliability and trustworthiness.
% avoiding spurious correlations and non-causal biases in the data.
% In high-risk applications, where performance guarantees beyond the training distribution are desirable, causal inference is critical.
% Causal inference is crucial for systematically identifying
% Causal models explain a system's behavior by elucidating the causal relationships among its components.
However, translating the rigorous theoretical tools of causality into practical methods, especially in the large-scale regime with heterogeneous unstructured data as in large models, remains a notable challenge, despite the growing attention by the community.
%, especially in high-risk applications such as medical diagnosis or autonomous driving.

% This opens up substantial potential for using causality to address key open questions about large models. By leveraging causal inference, we hope to tackle these questions rigorously and enhance our understanding of these powerful models, as well as their reliability and trustworthiness.

% \todo{highlight from the begining that large models are still not rubust enought, especially for distribution shifts. This is the main goal of the first 2 direction from bellow, and it will be good to introduce some relevant questions before. we could point to the fact that our theoretical understanding concerns mostly iid settings - and causality could be a good tool to move to OOD regimes.}\todo{Improving the reasoning abilities and reliabilities of the current model. The logit is Why(A) -> What(B) -> How(C), before (C), should we take some examples to make (C) more concrete like: Therefore, it is essential to consider and explore how to systematically improve the reasoning capabilities and reliability of large models from multiple perspectives, including model architecture, algorithm improvements, data diversity, evaluation systems, and transparency.}
% % (D) \textit{How to make large models more robust?}

% Enter causality: a principled framework for predicting a system's behavior under interventions and reasoning over counterfactual scenarios. \todo{Should we emphasise that:  
% Causality is a very powerful framework that can help us analyze, evaluate, and enhance the trustworthiness of systems.}
% In high-risk applications, where performance guarantees beyond the training distribution are desirable, causal inference is critical.
% Moreover, causal models explain a system's behavior by elucidating the causal relationships among its components.
% This opens up substantial potential for using causality to address key questions about large models, such as (A), (B), or (C). By leveraging causal inference, we hope to tackle these questions rigorously and enhance our understanding of these powerful models, as well as their reliability and trustworthiness.

% For example, by systematically identifying failure modes 
% or by isolating the model components that cause problematic behavior 
% we can gain insights into the causal factors that affect the model's behavior. 
% This can lead to more interpretable and explainable models, while also promising to improve model out-of-distribution generalization and robustness by avoiding spurious correlations and non-causal biases in the data.

% Our workshop will explore the many exciting synergies between causality and large models. 

With the striking potential of causality and the enormous interest in tackling the many open questions about understanding and improving large models on the other, we propose a workshop that aims to explore the many exciting synergies between causality and large models.
Specifically, we identify four main directions to cover in our workshop: 

% (1) Causality \textit{in} large models, (2) Causality \textit{for} large models, (3) Causality \textit{with} large models, and (4) Causality \textit{of} large models.

% In - Studying the causal knowledge captured by large models
%   - Evaluating the causal knowledge that emerges out of standard training schemes
%   - Do LMs do “amortized causality” (Bernhard’s term)?
%   - Related to: Psychometrics, behavioral sciences
%   - References: causal parrots, CLadder, Causal Reasoning in LLMs (2305.00050)
%   - Example application: holistic benchmarking
%   - Example approach: evaluate model biases with a synthetic benchmark
% - Of - Understanding the causal structure of how large models work
%   - Focus is on exploiting the structure of large models to make them more interpretable/controllable
%   - Related to: Mechanistic interpretability, alignment, neuroscience
%   - Example application: explainable/trustworthy AI
%   - Example approach: Identify what components of the model cause problematic behavior
% - For - Applying causal methods to improve large models
%   - ML, robustness, generalization, transfer methods
%   - Explicit or (often) implicit causal assumptions about the model/task
%   - “Climbing up the ladder” to improve our models
%   - Example approach: collecting interventional training data
%   - Example application: debiasing models
% - With - Leveraging large models to improve causal inference/discovery
%   - Core contribution is to causal inference/discovery
%   - How can the high performance of large models be exploited by causal methods?
%   - References: Can LLMs Build Causal Graphs (2303.05279), …
%   - Example application: causal discovery using LLMs
%   - Example approach: leveraging semantics learned in pre-training

\begin{enumerate}[nolistsep]
    \item \textbf{Causality \textit{in} large models}: Assessing the causal knowledge captured by large models and their (causal) reasoning abilities.
    % A growing area of research in this direction is to investigate the causal knowledge captured in large models despite being trained on unstructured, observational data.
    % amortized causal inference - causal knowledge *emerges* out of large-scale observational data 
    % how multiple modalities enable more causal reasoning (?)
    \item \textbf{Causality \textit{for} large models}: Applying ideas from causality to augment and improve large models.
    % Building off of efforts towards causal representation learning, we aim to share ideas on inductive biases inspired/grounded in causality for the large-scale regime
    \item \textbf{Causality \textit{with} large models}: Leveraging large models to improve causal inference and discovery.
    % is that recent large models appear to be able to perform some causal reasoning and extract some causal knowledge without deliberate or intended incorporation of causality in their conception.
    \item \textbf{Causality \textit{of} large models}: Investigating the causal structure of how large models work and how to make them more interpretable and controllable. 
    % Exploiting the rich framework of causal inference to understand the impressive capabilities of large models
\end{enumerate}

By engaging both theoretical and applied perspectives, we aim to foster deep insights into the behavior of large models, not only to assess their current capabilities but also to improve their performance and to make their behavior more interpretable and reliable. In addition to the directions discussed above, we welcome contributions that explore the vibrant interface between causality and large models, for example:

\begin{itemize}[nolistsep]
    \item Theoretical and empirical studies on how large models employ causal knowledge and reasoning
    \item Combined applications of large models and causal methods to real-world problems for improved performance and interpretability 
    \item Connections between multiple modalities (such as images, text, and audio) and (causal) reasoning abilities of large models
    \item Applications of causal inference methods to understand or improve large models
\end{itemize}

% [specific criteria (for call for papers)]


\paragraph{Schedule}

Our workshop will feature a mix of invited talks, a poster session, a panel discussion, and interactive elements to engage the audience and foster discussion. Consequently, we do not expect to need any special equipment.
We plan to hold a series of six invited talks given by both up-and-coming as well as distinguished researchers with experience in the intersection of causality and large models, as well as a panel discussion for deeper dialogue on the workshop's main themes. Overall, we estimate that our workshop will run for a full day (approximately 8-9 hours), with a preparatory rough breakdown as follows:


\begin{itemize}[nolistsep]
    \item 30 min: Opening and closing remarks
    \item 6 x 30 min: Invited talks
    \item 60 min: Panel discussion
    \item Up to 60 min: Contributed talks
    \item 60 min: Poster session (with up to an additional 60 min during coffee breaks)
    \item 90 min: Lunch break (with a voluntary ``lunch buddy'' program to connect participants)
    \item 2 x 30 min: Coffee breaks
\end{itemize}



% [lunch buddy program] to connect not just junior+senior people for lunch but also connecting different communities
    % To connect junior and senior researchers and simultaneously encourage cooperation between the causality and large model communities, we---inspired by the success of a similar initiative at the NeurIPS workshop NeurReps---plan to setup a ``lunch buddy" system between the participants, making our workshop more interactive.


Taking inspiration from a similar idea at the NeurIPS 2022 workshop NeurReps and to foster a more interactive and engaging environment, we plan to include a ``lunch buddy'' program to connect junior and senior researchers, as well as researchers from different communities, for lunch. 
Attendees as well as participants will be able to sign up for this program throughout the morning of the workshop before lunch, self-reporting their research interests and experience level, then we will randomly pair them up and provide them with a list of suggested topics to discuss during lunch.
This will provide an opportunity for participants to network and engage in discussions with researchers that have different backgrounds, but similar research interests.

% Our workshop will feature a mix of talks, panel discussions, and interactive elements to engage the audience and foster discussion. We plan to have a total of 6 talks, 2 panel discussions, and 1 interactive session. The schedule will be structured to allow for ample time for questions and discussion, as well as breaks to encourage networking and collaboration among the participants.


% We also selected several notable researchers that focus on adjacent or applied fields to provide our attendees with insights in how state-of-the-art methods can rapidly and effectively be deployed to solve problems.
% features a distinguished group of speakers and panelists. 

% We have invited {\color{blue} [potentially turn this into a bulletpoint list \& add affiliation/position? It would showcase the diversity in seniority \& institutions]}  
% {\color{green} [link speaker's website and short description?]}

\paragraph{Speakers and Panelists}

All our invited speakers and panelists have confirmed their in-person participation except one.\footnote{Matej Zečević, who is still pending confirmation}
We are very encouraged by the enthusiastic reception our proposed workshop has already received from the speakers and panelists we invited, with several speakers expressing their excitement at the topic. %and the opportunity to engage with the community.

% We are also in the process of finalizing the schedule, which will feature a mix of talks, panel discussions, and interactive elements to engage the audience and foster discussion.

% a great lineup of speakers and panellists from a wide seniority range, including academic and industry experts.

Our confirmed~\textit{speakers} are:%\footnote{All our speakers and panellists have confirmed their participation, except Matej Zečević.}  
\href{https://www.janexwang.com/}{Jane X. Wang} (staff research scientist, DeepMind),
\href{https://www.claudiajshi.com/}{Claudia Shi} (Ph.D.\ student, Columbia University),
\href{http://victorveitch.com/}{Victor Veitch} (Assistant professor, University of Chicago; research scientist, Google Brain),
\href{https://amitsharma.in/}{Amit Sharma} (Principal researcher, Microsoft Research),
\href{https://causalai.net/}{Elias Bareinboim} (Associate professor, Columbia University), 
\href{https://www.matej-zecevic.de/}{Matej Zečević} (Ph.D.\ student, TU Darmstadt; to be confirmed)

Our confirmed \textit{panellists} are:
\href{https://maria-antoniak.github.io/}{Maria Antoniak} (Young Investigator, Allen Institute for AI),
\href{https://sites.google.com/view/giambattista-parascandolo/home}{Giambattista Parascandolo} (Research scientist, OpenAI; tentatively accepted),
\href{https://atticusg.github.io/}{Atticus Geiger} (Principal investigator, Stanford University),
\href{https://www.bunnelab.com/}{Charlotte Bunne} (Assistant professor, EPFL),
\href{https://ai.stanford.edu/~cbfinn/}{Chelsea Finn} (Assistant professor, Stanford University) , and
\href{https://zhijing-jin.com/}{Zhijing Jin} (Ph.D.\ student, MPI-IS and ETH Zürich).


As our workshop endeavors to bring together two research sub-communities, large-scale machine learning and causal inference, we are fortunate to draw our speakers and panelists from diverse backgrounds, demographics, experience, and institutions.
We are excited to have a mix of senior and junior researchers, as well as a blend of academic and industry experts, to provide a multi-faceted perspective on causality and large models.
Additionally, to encourage a more inclusive and diverse environment, we have also selected several notable researchers who focus on adjacent or applied fields to provide our attendees with insights into how state-of-the-art methods can rapidly and effectively be deployed to solve real-world problems.


% [speakers represent industry perspective]
% {\color{cyan} see above}

% [note on diversity]
% {\color{cyan} see above}

% \subsection{Interactive Elements}

% [equipment needs]


\paragraph{Relevance and Impact}

Although there have been several past workshops focusing on various aspects of large models at NeurIPS and other major conferences, to our knowledge, there has not been a workshop that specifically focuses on the intersection between causality and large models.
This is particularly unfortunate, given the recent rise of large models leading to an explosion of attention in the research community and beyond. 

However, there has been a growing interest in making causality more accessible to the machine learning community and fostering research at the intersection between the two fields, as evidenced by several past Causal Representation Learning workshops. We hope to carry on this mission by discussing how causal methods can enable more principled approaches and potentially even new capabilities such as systematic generalization or robustness. However, we aim to focus on large models as they represent a paradigm shift in machine learning, not only in terms of performance but also the social implications of their increasingly widespread deployment.
% has brought new challenges and opportunities that are rife for deeper discussion

Since this is the inaugural workshop on this topic, it is difficult to estimate the number of contributions or attendees. However, judging by the 2022 UAI workshop on Causal Representation Learning which had 32 submissions, we expect up to 70 submissions, since NeurIPS is a larger conference, and our workshop involves the trending topic of large models. Estimating the number of attendees is also challenging, but judging by the reception by invited speakers and reviewers, we expect a high interest in the topic, so we estimate upwards of 200 attendees.

% , and anticipate around 150 attendees.


% [high application and industry cares a lot about tthis]

% [comparison to CRL] include stats on CRL workshops

% why now? (why is this workshop timely?)

% We hope to focus on the intersection between causality and large models in particular, as the paradigm shift towards large models has brought new challenges and opportunities that are rife for deeper discussion.

% Past related workshops, such as the Causal Representation Learning workshop at NeurIPS 2023, have strived to make causal methods more accessible to the machine learning community, and we hope to carry on this mission by discussing how a causal framing can enable more principled approaches and potentially even new capabilities such as systematic generalization or robustness. However, 
% For example, how this knowledge is utilized for "amortized causal reasoning" for solving complex tasks.

% crystallize emerging subfields
% 

% [list of "selected references"]

% C♥LM is timely amidst the safety and robustness concerns of AI in high-stakes settings, and that causality is an important angle to consider

% how causal understanding is fundamental for achieving robust performance in multi-modal, healthcare settings.


\section{Team}

Our organizing team is very fortunate to benefit from not only considerable experience both in breadth and depth, but also in exceptional diversity in terms of research interests, backgrounds, gender, and demographics. With 12 members spanning six different affiliations across six different countries in North America, Europe, and Asia, we are well-positioned to bring together a diverse set of perspectives and expertise to our workshop.

Despite this commendable diversity, our team is larger than recommended by the NeurIPS guidelines. This is partially due to multiple different groups independently working towards organizing a workshop on this topic, which we decided to merge into a single united effort. All our organizers can devote their full attention to making the workshop rewarding for contributors, speakers, and attendees, as no organizers are additionally organizing on other workshops in parallel.
However, we believe that for this topic, the size of our team is not only appropriate, but also beneficial. 
By bringing six institutions together, we have access to a large pool of potential reviewers ensuring that each submission receives at least three quality reviews, enabling us to, by our estimates, already have about one-third of the necessary reviewers confirmed.
Additionally, our organizing team includes several junior researchers and a high variance in experience organizing workshops in the past. This makes this workshop a unique opportunity for many students to gain valuable experience in organizing a workshop at a major conference.

% we can draw from a wider range of expertise and facilitate knowledge transfer regarding the best practices of organizing workshops.
% We believe that
% the benefits of having a larger team outweigh the potential drawbacks, as it allows us to draw on a wider range of expertise 
% This enables us to have access to a larger and more diverse set of reviewers. Furthermore, our organizing team facilitates knowledge transfer regarding the best practices of organizing workshops, as it includes several junior people for whom this is the first experience as an organizer. 

% [number of organizers] dedicated specialists (deep bench of experience); we are bringing two different communities together. organizers need to cover not just one but two communities (and everything in between); knowledge transfer from experienced organizers; larger pool of reviewers to pull from (more institutions/groups involved)
% Our workshop brings together two communities, which requires experts from both as organizers. 

\subsection{Organizer Introductions}

\href{https://felixludos.com/}{Felix Leeb} 
is a senior Ph.D.\ student at the Max Planck Institute for Intelligent Systems in Germany, under the supervision of Bernhard Schölkopf working on causal representation learning and evaluating the reasoning abilities of large language models. Before living in Germany, Felix completed his Bachelors in Physics, Chemistry, and Computer Science at the University of Washington in Seattle, USA, researching object pose estimation visuomotor control and using molecular dynamics simulations to study the structure of cell membranes.

\href{https://chinglamchoi.github.io/cchoi/}{Ching Lam Choi} is an incoming Ph.D.\ student at MIT EECS; she has also spent time with brilliant mentors and peers at Mila, Max Planck Institute for Intelligent Systems, University of Copenhagen, CUHK MMLab and more. Her experiences inspire her to advocate for diverse research participation, via hosting open office hours and co-organising ``New in ML Workshop" at NeurIPS 2023, ``CoSubmitting Summer Workshop" at ICLR 2022, and ``Undergraduates in Computer Vision Social" in ICCV 2021. Her research interests are in responsibly robust scaling, through better synthetic data and causal reasoning capabilities.

\href{https://lgresele.github.io/}{Luigi Gresele}  is a postdoctoral researcher at the University of Copenhagen. His research focuses on the intersection of representation learning and causal inference. Luigi earned his Ph.D.\ from the Max Planck Institute for Intelligent Systems and the University of Tübingen, under the supervision of Bernhard Schölkopf. During his Ph.D.\, he took part in an ELLIS exchange with the Parietal team at Inria-CEA, hosted by Bertrand Thirion and Aapo Hyvärinen, and worked as an Applied Science Intern at Amazon Research Tübingen with Dominik Janzing. He co-organised the 2022 Workshop on Causal Representation Learning (CRL) at UAI Eindhoven, and the 2023 CIFAR and ELLIS Workshop on CRL in T\"ubingen.

\href{https://andreinicolicioiu.github.io/}{Andrei Nicolicioiu} is a Ph.D.\ student at Mila and Universit\'{e} de Montr\'{e}al, Canada supervised by Aaron Courville with a Bachelor’s and a Master’s degree from the University Politehnica of Bucharest, Romania. Previously he worked as an ML researcher for 5 years at Bitdefender, Romania, and spent some time at the Max Planck Institute for Intelligent Systems, Germany. His research involves vision-language models with a focus on visual relational reasoning, out-of-distribution generalization and causally inspired representation learning, and data-selection methods.

Xiusi Li is a Master's student at Mila and McGill University supervised by Siamak Ravankbash. She has previously completed her integrated master's degree in Mathematics at the University of Oxford and worked as an ML engineer for 2 years at a biomedical startup in London. Her research interests include causal representation learning and more generally the use of symmetry in machine learning.

Sophie Xhonneux is a Ph.D.\ student at Mila and Universit\'{e} de Montr\'{e}al, Canada supervised by Gauthier Gidel and Jian Tang. She completed her a Bachelor's and a Master's degree in Computer Science at the University of Cambridge. Her research interest include robust and aligned large language models, graph representation learning, reasoning, and out-of-distribution generalisation.


\href{https://rpatrik96.github.io/}{Patrik Reizinger} is a Ph.D.\ student at the Max Planck Institute for Intelligent Systems in Germany, supervised by Wieland Brendel, Ferenc Huszár, Matthias Bethge, and Bernhard Schölkopf. He is part of the ELLIS and IMPRS-IS programs. Before his Ph.D., he got a Bachelor's and Master's degree in electrical engineering from the Budapest University of Technology, where he specialized in control engineering and intelligent systems. His main research interests include causal representation learning and identifiability, which he also applies to understand emergent behavior in LLMs. He is currently visiting the Vector Institute in Toronto, Canada, hosted by Rahul G.~Krishnan.

\href{https://valvoda.github.io/}{Josef Valvoda} is a postdoctoral researcher at the University of Copenhagen. 
Before starting his postdoc, Josef obtained his Ph.D. from the University of Cambridge, where he was co-advised by Ryan Cotterell (ETH) and Simone Teufel (Cambridge).
His work is focused on formalizing legal reasoning---he has completed his undergraduate degree in Law---as well as the study of neural models through formal language theory.
He has previously interned as an Applied Scientist at Amazon, ML/AI researcher at Apple and will be joining Google X for a residency this summer.

\href{https://scholar.google.com/citations?user=gtDqiucAAAAJ&hl=en}{Haoxuan Li} is a Ph.D. student in Center for Data Science, Peking University. His research interests span from causal machine learning theory, counterfactual fairness, recommender system debiasing, out-of-distribution generalization, multi-source data fusion, bioinformatics, and large language models. He has published more than 20 papers as first author in top conference
proceedings, including ICML, NeurIPS, ICLR, SIGKDD, WWW, SIGIR, ICDE, AAAI, and IJCAI, etc., and recieved the NSFC Young Scientists Fund (2024). Moreover, he has been served as the PC member (or area chair) for several top conferences including ICML, NeurIPS, ICLR, SIGKDD, WWW, AAAI, IJCAI, and MM, also the invited reviewer for prestigious journals such as The Innovation, TOIS, TKDE, TKDD, TNNLS, and IPM.

\href{https://ymy4323460.github.io/}{Mengyue Yang} is a Ph.D. student from University College London, supervised by Professor Jun Wang. Her research interests are causality, multi-agent systems, reinforcement learning and human-AI coordination. She was co-organizer of the causal representation learning workshop at ICDM 2024 and Competition Causal Structure Learning from Event Sequences and Prior Knowledge at NeurIPS 2023.

% [specialized teams and responsibilities]

% [program committee and discussion of reviewing process]


\subsection{Program Committee}

We are also happy to announce that, we have already confirmed 34 reviewers for our workshop, based on a voluntary form sent out internally at two of the six institutions represented by our organizing committee. Although this is already a substantial number of reviewers, at this early stage, we will continue to reach out to nominate additional qualified reviewers 
to ensure a fair and thorough reviewing process.
Several of the organizers will serve as area chairs to oversee the reviewing process.

% For our program committee, we have sent out invitations internally at our institutions and to our collaborators, and we are in the process of finalizing the committee. We aim to have a diverse and inclusive program committee to ensure a fair and thorough reviewing process.


\section{Selected References}

Below we provide a list of selected references for potential attendees and contributors as well as reviewers to get a better understanding of the topics we aim to cover in our workshop. As this is a very active area of research, we expect several pertinent papers to be published in the coming months, so we will provide a continuously updating list at our workshop's website: \url{https://calm-workshop-2024.github.io/}

% {\color{blue}[Whenever possible, I would suggest adding at least one reference for each of the invited speakers/panelists/organisers, to showcase that each is a good fit]}

\bibliographystyle{unsrtnat}
\renewcommand{\bibsection}{}
\bibliography{refs}

\nocite{liuLargeLanguageModels2024,
yang2023critical,
anwar2024foundational, jin2024cladder, kasetty2024evaluating,park2023linear,geiger2024finding,nie2024moca,zevcevic2023causal,vashishtha2023causal, reizinger2024understanding, willig2022can, xia2021causal, d2022underspecification, long2023causal, rajendran2024learning, jiang2024origins, wang2024concept}
\nocite{gupta2024context_is_env, lampinen2023passive, wu2024interpretability}
\nocite{kiciman2023causal}
\nocite{montagna2024demystifying}
\nocite{daunhawer2023identifiability}
\nocite{sanchez2022diffusion}
\nocite{pan2024counterfactual, li2024steering}
\nocite{abdulaal2024causal}
\nocite{richens2024robust}
\nocite{feder2024causal}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \newpage

% \appendix

% \section{Appendix / supplemental material}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}